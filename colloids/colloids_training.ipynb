{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gc\n",
    "from tqdm import tqdm,trange\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torchani\n",
    "import os\n",
    "import math\n",
    "from torch.nn import ModuleList, Sequential\n",
    "from collections import OrderedDict\n",
    "from torch import Tensor\n",
    "from typing import Tuple, NamedTuple, Optional\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load stationary samples within the reactant (A), product (B) and the region outside of it (C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flA  = '../jupyter_data/colloid_A.npy'\n",
    "flB  = '../jupyter_data/colloid_B.npy'\n",
    "flC  = '../jupyter_data/colloid_C.npy'\n",
    "\n",
    "datA = np.load(flA).astype(np.float32)\n",
    "datB = np.load(flB).astype(np.float32)\n",
    "datC = np.load(flC).astype(np.float32)\n",
    "\n",
    "NA = datA.shape[0]\n",
    "NB = datB.shape[0]\n",
    "NC = datC.shape[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Divide the data into batches for the training set and validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Nbatch = 10\n",
    "Nval = 5000\n",
    "\n",
    "#Number of samples per batch\n",
    "NsampC = int((4.0e4)/Nbatch)\n",
    "NsampA = NsampC\n",
    "NsampB = NsampC\n",
    "\n",
    "#Obtain randomized sample indices for each dataset\n",
    "idA = np.random.choice(NA,(Nbatch*NsampA+Nval),replace=False)\n",
    "idB = np.random.choice(NB,(Nbatch*NsampB+Nval),replace=False)\n",
    "idC = np.random.choice(NC,(Nbatch*NsampC+Nval),replace=False)\n",
    "\n",
    "#Define the validation dataset and the species holder\n",
    "xtA_val = torch.tensor(np.concatenate([datA[:Nval],np.zeros((Nval,7,1))],axis=-1)).to(device).float()\n",
    "xtB_val = torch.tensor(np.concatenate([datB[:Nval],np.zeros((Nval,7,1))],axis=-1)).to(device).float()\n",
    "xtC_val = torch.tensor(np.concatenate([datC[:Nval],np.zeros((Nval,7,1))],axis=-1)).to(device).float().requires_grad_(True)\n",
    "species_val = torch.tensor(np.tile(np.array([0,1,1,1,1,1,1],dtype=int),(Nval,1))).to(device).int()\n",
    "\n",
    "#Define the training dataset and the species holder\n",
    "idA = idA[Nval:].reshape(Nbatch,NsampA)\n",
    "idB = idB[Nval:].reshape(Nbatch,NsampB)\n",
    "idC = idC[Nval:].reshape(Nbatch,NsampC)\n",
    "\n",
    "xtA = torch.tensor(np.array([np.concatenate([datA[idA[i]],np.zeros((NsampA,7,1))],axis=-1) \n",
    "                    for i in range(Nbatch)])).to(device).float()\n",
    "xtB = torch.tensor(np.array([np.concatenate([datB[idB[i]],np.zeros((NsampA,7,1))],axis=-1) \n",
    "                    for i in range(Nbatch)])).to(device).float()\n",
    "xtC = [torch.tensor(np.concatenate([datC[idC[i]],np.zeros((NsampA,7,1))],axis=-1))\n",
    "        .to(device).float().requires_grad_(True) for i in range(Nbatch)]\n",
    "species = torch.tensor(np.tile(np.array([0,1,1,1,1,1,1],dtype=int),(NsampC,1))).to(device).int()\n",
    "\n",
    "#Define parameters for the PBC information\n",
    "L = 8.\n",
    "cell = np.zeros((3,3))\n",
    "np.fill_diagonal(cell,L)\n",
    "cell = torch.tensor(cell).to(device).float()\n",
    "pbc = torch.tensor(np.array([True,True,False])).to(device).bool()\n",
    "\n",
    "#Delete the temporary dataset files\n",
    "del idA,idB,idC,datA,datB,datC\n",
    "\n",
    "gc.collect();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the modified TorchANI and Sequential model for training the committor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpeciesEnergies(NamedTuple):\n",
    "    species: Tensor\n",
    "    energies: Tensor\n",
    "\n",
    "\n",
    "class SpeciesCoordinates(NamedTuple):\n",
    "    species: Tensor\n",
    "    coordinates: Tensor\n",
    "    \n",
    "class ANIModel1(torch.nn.ModuleDict):\n",
    "    @staticmethod\n",
    "    def ensureOrderedDict(modules):\n",
    "        if isinstance(modules, OrderedDict):\n",
    "            return modules\n",
    "        od = OrderedDict()\n",
    "        for i, m in enumerate(modules):\n",
    "            od[str(i)] = m\n",
    "        return od\n",
    "\n",
    "    def __init__(self, modules):\n",
    "        super().__init__(self.ensureOrderedDict(modules))\n",
    "\n",
    "    def forward(self, species_aev: Tuple[Tensor, Tensor],  # type: ignore\n",
    "                cell: Optional[Tensor] = None,\n",
    "                pbc: Optional[Tensor] = None) -> SpeciesEnergies:\n",
    "        species, aev = species_aev\n",
    "        \n",
    "        assert species.shape == aev.shape[:-1]\n",
    "        \n",
    "        atomic_energies = self._atomic_energies((species, aev))\n",
    "\n",
    "        return SpeciesEnergies(species, torch.sum(atomic_energies, dim=1))\n",
    "\n",
    "\n",
    "    @torch.jit.export\n",
    "    def _atomic_energies(self, species_aev: Tuple[Tensor, Tensor]) -> Tensor:\n",
    "\n",
    "        species, aev = species_aev\n",
    "        assert species.shape == aev.shape[:-1]\n",
    "        species_ = species.flatten()\n",
    "        aev = aev.flatten(0, 1)\n",
    "        \n",
    "        output = aev.new_zeros([species_.shape[0],10])\n",
    "        tmp_species_ = torch.tile(species.flatten()[:,None],(1,10))\n",
    "        sp = torch.tile(species[:,:,None],(1,1,10))\n",
    "\n",
    "        for i, m in enumerate(self.values()):\n",
    "            \n",
    "            mask = (tmp_species_ == i)\n",
    "            midx = mask[...,0].nonzero().flatten()\n",
    "\n",
    "            tot = (mask.sum()/species.shape[0]/10).int()\n",
    "            \n",
    "            if midx.shape[0] > 0:\n",
    "                input_ = aev.index_select(0, midx)\n",
    "                output.masked_scatter_(mask, m(input_).reshape(species.shape[0]*tot,10))\n",
    "\n",
    "        output = output.view_as(sp)\n",
    "\n",
    "        return output\n",
    "\n",
    "class Sequential1(torch.nn.ModuleList):\n",
    "\n",
    "    def __init__(self, *modules):\n",
    "        super().__init__(modules)\n",
    "\n",
    "    def forward(self, input_: Tuple[Tensor, Tensor],  # type: ignore\n",
    "                cell: Optional[Tensor] = None,\n",
    "                pbc: Optional[Tensor] = None):\n",
    "        z = 0\n",
    "        for module in self:\n",
    "            if z<2:\n",
    "                input_ = module(input_, cell=cell, pbc=pbc)\n",
    "            elif z == 2:\n",
    "                input_ = module(input_[1])\n",
    "            else:\n",
    "                input_ = module(input_)\n",
    "            z += 1\n",
    "\n",
    "        return input_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the AEV descriptors\n",
    "aev_computer = torchani.AEVComputer.cover_linearly(4*np.sqrt(2), 2., 16.0, 8.0, 30, 3, 32.0, 6, 2)\n",
    "aev_dim = aev_computer.aev_length\n",
    "\n",
    "#Network for the red particle\n",
    "red_network = torch.nn.Sequential(\n",
    "    torch.nn.Linear(aev_dim, 5),\n",
    "    torch.nn.CELU(0.1),\n",
    "    torch.nn.Linear(5, 10),\n",
    "    torch.nn.CELU(0.1),\n",
    "    torch.nn.Linear(10, 10),\n",
    ")\n",
    "\n",
    "#Network for the black particle\n",
    "black_network = torch.nn.Sequential(\n",
    "    torch.nn.Linear(aev_dim, 5),\n",
    "    torch.nn.CELU(0.1),\n",
    "    torch.nn.Linear(5, 10),\n",
    "    torch.nn.CELU(0.1),\n",
    "    torch.nn.Linear(10, 10),\n",
    ")\n",
    "\n",
    "# Initialize the ANI and the Sequential model\n",
    "nn = ANIModel1([red_network,  black_network])\n",
    "model = Sequential1(aev_computer, nn\n",
    "                    ,torch.nn.Tanh(),torch.nn.Linear(10,1),\n",
    "                    torch.nn.Sigmoid()).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize the model parameters and the validation method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize the weights and biases of the model\n",
    "def init_params(m):\n",
    "    if isinstance(m, torch.nn.Linear):\n",
    "        torch.nn.init.kaiming_normal_(m.weight, a=1.0)\n",
    "        torch.nn.init.zeros_(m.bias)\n",
    "\n",
    "nn.apply(init_params);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compute the validation set loss\n",
    "def validate():\n",
    "    model.train(False)\n",
    "\n",
    "    committorA = model((species_val,xtA_val),pbc=pbc,cell=cell)\n",
    "    committorB = model((species_val,xtB_val),pbc=pbc,cell=cell)\n",
    "    committorC = model((species_val,xtC_val),pbc=pbc,cell=cell)\n",
    "\n",
    "    lamb = torch.autograd.grad(committorC.sum(), xtC_val, create_graph=True, retain_graph=True)[0][...,:-1]\n",
    "\n",
    "    lossBoundary = reg*(torch.square(committorA).mean()\n",
    "                        + torch.square(1-committorB).mean())\n",
    "    lossBKE = torch.square(lamb).sum()/Nval\n",
    "\n",
    "    loss = lossBoundary + lossBKE \n",
    "\n",
    "    model.train(True)\n",
    "    return lossBKE,lossBoundary\n",
    "gc.collect();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AdamW = torch.optim.Adam(model.parameters(),lr=1e-3,weight_decay=0.001) #Optimizer\n",
    "Nepochs = 1000 # Number of epochs\n",
    "reg = 5000. # Lagrange multipliers for the boundary conditions\n",
    "losses = np.zeros((Nepochs,4)) #Array for saving the running loss\n",
    "\n",
    "# Iterator for optimization \n",
    "tqs= trange(Nepochs,desc=\"Loss : \",leave=True)\n",
    "desc = 'Tr BKE : {: .3f} | Tr Bdy : {: .3f} | Val BKE: {:.3f} | Val Bdy: {:.3f}'\n",
    "\n",
    "for i in tqs:\n",
    "    # Reset the optimizer after 500 steps\n",
    "    if i % 500:\n",
    "        AdamW = torch.optim.Adam(model.parameters(),lr=1e-3,weight_decay=0.0001)\n",
    "        \n",
    "    #Compute the validation loss\n",
    "    val1,val2 = validate() \n",
    "    losses[i][2] = val1.cpu().detach().numpy()\n",
    "    losses[i][3] = val2.cpu().detach().numpy()\n",
    "    \n",
    "\n",
    "    # Randomized iterator for selecting the batches\n",
    "    rangeA = np.random.choice(Nbatch,Nbatch,replace=False)\n",
    "    rangeB = np.random.choice(Nbatch,Nbatch,replace=False)\n",
    "    rangeC = np.random.choice(Nbatch,Nbatch,replace=False)\n",
    "    \n",
    "    \n",
    "    for j,k,l in zip(rangeC,rangeB,rangeA):\n",
    "        \n",
    "        #Compute the boundary loss on the A and B samples\n",
    "        committorA = model((species, xtA[l]),pbc=pbc,cell=cell)\n",
    "        committorB = model((species, xtB[k]),pbc=pbc,cell=cell)\n",
    "        lossBoundary = reg*(torch.square(committorA).sum()/NsampA\n",
    "                            + torch.square(1-committorB).sum()/NsampB)\n",
    "        \n",
    "        #Compute BKE loss on the C samples\n",
    "        committorC = model((species, xtC[j]),pbc=pbc,cell=cell)\n",
    "        lamb = torch.autograd.grad(committorC.sum(), xtC[j], create_graph=True,\n",
    "                                   retain_graph=True)[0][...,:-1]\n",
    "        lossBKE = torch.square(lamb).sum()/NsampC\n",
    "\n",
    "        #Compute total loss and perform backpropogation\n",
    "        loss = lossBoundary + lossBKE \n",
    "        AdamW.zero_grad()\n",
    "        loss.backward()\n",
    "        AdamW.step()\n",
    "        \n",
    "        #Store the loss\n",
    "        losses[i][0] += lossBKE.cpu().detach().numpy()/Nbatch\n",
    "        losses[i][1] += lossBoundary.cpu().detach().numpy()/Nbatch\n",
    "        \n",
    "        tqs.set_description(desc.format(np.log(lossBKE.cpu().detach().numpy()),\n",
    "                                np.log(lossBoundary.cpu().detach().numpy()),\n",
    "                                np.log(val1.cpu().detach().numpy()),\n",
    "                                np.log(val2.cpu().detach().numpy())),refresh=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model,'../jupyter_data/colloids_committor.pt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "HOOMD",
   "language": "python",
   "name": "hoomd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
